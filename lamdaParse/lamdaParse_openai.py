import os
from dotenv import load_dotenv
from llama_cloud_services import LlamaParse
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# .envì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
llama_api_key = os.getenv("LLAMA_CLOUD_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

if not llama_api_key:
    raise ValueError("âŒ LLAMA_CLOUD_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!")
if not openai_api_key:
    raise ValueError("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!")

# LlamaParse ì„¤ì • (Markdown ë³€í™˜)
parser = LlamaParse(result_type="markdown", premium_mode=True)

# PDF íŒŒì¼ ê²½ë¡œ
pdf_path = "pdf/ëª¨ë‹ˆí„°8p.pdf"

# PDF ë¬¸ì„œ íŒŒì‹± (Markdownìœ¼ë¡œ ë³€í™˜)
file_extractor = {".pdf": parser}
documents = SimpleDirectoryReader(input_files=[pdf_path], file_extractor=file_extractor).load_data()

# OpenAI LLM ë° Embeddings ì„¤ì •
llm = OpenAI(api_key=openai_api_key)
embedding = OpenAIEmbedding(api_key=openai_api_key)

# ë¬¸ì„œ ì¸ë±ì‹±
index = VectorStoreIndex.from_documents(documents, llm=llm, embed_model=embedding)
query_engine = index.as_query_engine()

# ìƒ˜í”Œ ì§ˆì˜ ìˆ˜í–‰
query = "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
response = query_engine.query(query)
print("ğŸ” ì§ˆì˜ ì‘ë‹µ ê²°ê³¼:", response)
