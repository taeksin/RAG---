import os
import time
import numpy as np
import pandas as pd
import faiss
import json
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
from openai import OpenAI  # Upstage API ì‚¬ìš©
from concurrent.futures import ThreadPoolExecutor, as_completed

MAX_TOKENS = 4000  # ìµœëŒ€ ë¬¸ì ìˆ˜ ê¸°ì¤€

def split_text_into_chunks_by_chars(text, max_length=MAX_TOKENS):
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬, ìµœëŒ€ max_length ì´í•˜ì˜ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    return [text[i:i+max_length] for i in range(0, len(text), max_length)]

def get_embedding_dimension(client, model):
    """
    ì§§ì€ í…ìŠ¤íŠ¸("test")ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ì°¨ì›ì„ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        response = client.embeddings.create(model=model, input=["test"])
        return len(response.data[0].embedding)
    except Exception as e:
        print(f"ğŸš¨ ì„ë² ë”© ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return None

def get_embedding_for_text(text, client, model, dim, max_retries=3):
    """
    ê°œë³„ í…ìŠ¤íŠ¸(text)ì— ëŒ€í•´ ì„ë² ë”© APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ MAX_TOKENS(ë¬¸ì ìˆ˜ ê¸°ì¤€)ë³´ë‹¤ ê¸¸ë©´ ì²­í¬ë¡œ ë¶„í•  í›„ ê° ì²­í¬ ì„ë² ë”©ì˜ í‰ê· ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨ ì‹œ max_retries ë§Œí¼ ì¬ì‹œë„í•˜ë©°, ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ 0 ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    def call_embedding(input_text):
        for attempt in range(max_retries):
            try:
                response = client.embeddings.create(model=model, input=[input_text])
                return response.data[0].embedding
            except Exception as e:
                error_str = str(e)
                # print(f"ğŸš¨ ì„ë² ë”© ìš”ì²­ ì‹¤íŒ¨ (ê°œë³„ ì²˜ë¦¬, ì‹œë„ {attempt+1}): {error_str}")
                # 429 ì˜¤ë¥˜ì´ë©´ ì§€ì •í•œ ëŒ€ê¸° ì‹œê°„ ì ìš©
                if '429' in error_str:
                    if attempt == 0:
                        delay = 9  # 3Â²=9ì´ˆ
                    elif attempt == 1:
                        delay = 20  # 3Â³=27ì´ˆ
                    else:
                        delay = 30
                    time.sleep(delay)
                else:
                    time.sleep(1)
        return None

    # í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ MAX_TOKENSë³´ë‹¤ í¬ë©´ ì²­í¬ë¡œ ë¶„í•  í›„ ê° ì²­í¬ ì„ë² ë”©ì„ ë³‘ë ¬(ë™ì‹œ ì‹¤í–‰ ìµœëŒ€ 2ê°œ)ë¡œ ìš”ì²­
    if len(text) > MAX_TOKENS:
        chunks = split_text_into_chunks_by_chars(text, MAX_TOKENS)
        sub_embeddings = []
        with ThreadPoolExecutor(max_workers=min(len(chunks), 2)) as executor:
            futures = [executor.submit(call_embedding, chunk) for chunk in chunks]
            for future in as_completed(futures):
                result = future.result()
                if result is not None:
                    sub_embeddings.append(result)
        if sub_embeddings:
            avg_embedding = np.mean(np.array(sub_embeddings), axis=0).tolist()
            return avg_embedding
        else:
            return [0.0] * dim
    else:
        emb = call_embedding(text)
        return emb if emb is not None else [0.0] * dim

def get_upstage_embedding_parallel(texts, client, model, dim, max_workers=2):
    """
    texts: ë¬¸ì„œë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ê° í–‰ ë‹¨ìœ„)
    ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ get_embedding_for_text()ë¥¼ ë™ì‹œ ì‹¤í–‰ ìµœëŒ€ max_workers ê°œë¡œ í˜¸ì¶œí•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    embeddings = [None] * len(texts)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {executor.submit(get_embedding_for_text, text, client, model, dim): idx for idx, text in enumerate(texts)}
        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                embeddings[idx] = future.result()
            except Exception as e:
                print(f"ğŸš¨ ì„ë² ë”© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ (ì¸ë±ìŠ¤ {idx}): {str(e)}")
                embeddings[idx] = [0.0] * dim
    return embeddings

# UpstageEmbeddings í´ë˜ìŠ¤ (Embeddings ê°ì²´ êµ¬í˜„)
from langchain.embeddings.base import Embeddings

class UpstageEmbeddings(Embeddings):
    def __init__(self, client, model, dim):
        self.client = client
        self.model = model
        self.dim = dim

    def embed_documents(self, texts):
        return get_upstage_embedding_parallel(texts, self.client, self.model, self.dim, max_workers=2)

    def embed_query(self, text):
        return self.embed_documents([text])[0]

def upstageEmbedding(folder_path):
    """
    folder_path ë‚´ë¶€ì— ìˆëŠ” ëª¨ë“  Excel íŒŒì¼(.xlsx)ì„ ê°œë³„ë¡œ ì²˜ë¦¬í•˜ì—¬
    Upstage APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ê° Excel íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ë¥¼ ì‚¬ìš©í•˜ë©°, íŒŒì¼ë³„ë¡œ ë³„ë„ì˜ ë²¡í„°ìŠ¤í† ì–´ê°€ ìƒì„±ë©ë‹ˆë‹¤.
    íŒŒì¼ ê°„ ë‚´ìš©ì´ ì„ì´ì§€ ì•Šë„ë¡ ì²˜ë¦¬í•˜ë©°, ë¬¸ì„œë³„ ì„ë² ë”© ìš”ì²­ì€ ë™ì‹œ ì‹¤í–‰ ìµœëŒ€ 2ê°œë¡œ ì œí•œë©ë‹ˆë‹¤.
    """
    load_dotenv()

    # Upstage API í‚¤ ì„¤ì •
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        print("UPSTAGE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # Upstage API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = OpenAI(
        api_key=api_key,
        base_url="https://api.upstage.ai/v1/solar"
    )

    # ì„ë² ë”© ì°¨ì› í™•ì¸
    dim = get_embedding_dimension(client, "embedding-passage")
    if dim is None:
        print("ì„ë² ë”© ì°¨ì›ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # folder_path ë‚´ì˜ ëª¨ë“  Excel íŒŒì¼ ì°¾ê¸°
    excel_files = []
    for f in os.listdir(folder_path):
        if f.lower().endswith('.xlsx'):
            file_path = os.path.join(folder_path, f)
            try:
                xl = pd.ExcelFile(file_path, engine='openpyxl')
                sheets = xl.sheet_names
                excel_files.append((file_path, sheets))
            except Exception as e:
                print(f"Excel íŒŒì¼ {f} ì½ê¸° ì‹¤íŒ¨: {e}")

    if not excel_files:
        print("ì²˜ë¦¬í•  Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê° Excel íŒŒì¼ë³„ë¡œ ë¬¸ì„œ(Document) ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì„ë² ë”© ì²˜ë¦¬ (í–‰ ë‹¨ìœ„)
    for file, sheets in excel_files:
        file_documents = []
        for sheet in sheets:
            try:
                df = pd.read_excel(file, engine='openpyxl', sheet_name=sheet)
            except Exception as e:
                print(f"ì‹œíŠ¸ {sheet} ì½ê¸° ì‹¤íŒ¨: {e}")
                continue

            if "content" not in df.columns or "metadata" not in df.columns:
                print(f"íŒŒì¼ {file}ì˜ ì‹œíŠ¸ {sheet}ì— 'content' ë˜ëŠ” 'metadata' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue

            df = df.dropna(subset=["content", "metadata"])
            for index, row in df.iterrows():
                content = row["content"]
                # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
                for pattern in ["[[[[[[ì´ì „ì²­í¬]", "[[[[[[í˜„ì¬ì²­í¬]", "[[[[[[ë‹¤ìŒì²­í¬]",
                                "[[[[[[í˜„ì¬í˜ì´ì§€ ì „ì²´ë‚´ìš©]", "[[[[[[ì´ì „í˜ì´ì§€ ë§ˆì§€ë§‰ ì²­í¬]", "[[[[[[ë‹¤ìŒí˜ì´ì§€ ì²«ë²ˆì§¸ ì²­í¬]"]:
                    content = content.replace(pattern, "")
                try:
                    metadata = json.loads(row["metadata"])
                except Exception as e:
                    print(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ (íŒŒì¼ {file}, ì‹œíŠ¸ {sheet}, í–‰ {index}): {e}")
                    continue
                if "text" in metadata:
                    for pattern in ["[[[[[[ì´ì „ì²­í¬]", "[[[[[[í˜„ì¬ì²­í¬]", "[[[[[[ë‹¤ìŒì²­í¬]",
                                    "[[[[[[í˜„ì¬í˜ì´ì§€ ì „ì²´ë‚´ìš©]", "[[[[[[ì´ì „í˜ì´ì§€ ë§ˆì§€ë§‰ ì²­í¬]", "[[[[[[ë‹¤ìŒí˜ì´ì§€ ì²«ë²ˆì§¸ ì²­í¬]"]:
                        metadata["text"] = metadata["text"].replace(pattern, "")
                file_documents.append(Document(page_content=content, metadata=metadata))
        
        if not file_documents:
            print(f"íŒŒì¼ {file}ì—ì„œ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # ê° Excel íŒŒì¼ ë‚´ ê° í–‰(ë¬¸ì„œ)ì— ëŒ€í•´ ë™ì‹œ ì‹¤í–‰ ìµœëŒ€ 2ê°œë¡œ ì„ë² ë”© ìš”ì²­
        file_text_contents = [doc.page_content for doc in file_documents]
        file_embeddings = get_upstage_embedding_parallel(file_text_contents, client, "embedding-passage", dim, max_workers=2)
        if len(file_embeddings) != len(file_documents):
            print("ğŸš¨ ì„ë² ë”© ê°œìˆ˜ê°€ ë¬¸ì„œ ê°œìˆ˜ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        file_index = faiss.IndexFlatL2(dim)
        try:
            file_index.add(np.array(file_embeddings, dtype=np.float32))
        except Exception as e:
            print(f"FAISS ì¸ë±ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            continue

        file_index_to_docstore_id = {i: str(i) for i in range(len(file_documents))}
        file_docstore = {str(i): doc for i, doc in enumerate(file_documents)}

        upstage_embedding_obj = UpstageEmbeddings(client, "embedding-passage", dim)
        file_vectorstore = FAISS(
            embedding_function=upstage_embedding_obj,
            index=file_index,
            docstore=file_docstore,
            index_to_docstore_id=file_index_to_docstore_id
        )

        excel_filename = os.path.splitext(os.path.basename(file))[0]
        save_path = os.path.join("vdb", "upstage_passage", f"{excel_filename}_embedding-passage")
        file_vectorstore.save_local(save_path)
        print(f"â•‘   -> {excel_filename} íŒŒì¼ì˜ ì„ë² ë”©ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    folder_path = r"C:\Users\yoyo2\fas\RAG_Pre_processing\data\250331-16-57_ëª¨ë‹ˆí„°1p"
    print("â•‘ âœ… upstageEmbedding ì‹œì‘")
    upstageEmbedding(folder_path)
